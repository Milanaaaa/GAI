{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ronz0a_GJ6Vf"
      },
      "source": [
        "# **Wasserstein GAN with Gradient Penalty (WGAN-GP)**\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Wasserstein GAN with Gradient Penalty (**WGAN-GP**) [WGAN Paper](https://arxiv.org/pdf/1701.07875), [WGAN-GP Paper](https://arxiv.org/pdf/1704.00028) represents an improvement over traditional GAN architectures by tackling problems of training instability and mode collapse. WGAN-GP utilizes the **Wasserstein distance** instead of standard GAN losses, which promotes more stable training dynamics and improved gradient behavior.\n",
        "\n",
        "A key innovation in WGAN-GP is the **Gradient Penalty (GP)** mechanism. GP replaces the less effective weight clipping method for enforcing the crucial **1-Lipschitz constraint**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1️⃣ Why Wasserstein Distance?**\n",
        "\n",
        "Conventional GANs often rely on **Jensen-Shannon (JS) divergence** to quantify differences between real and generated data distributions.  However, this approach can lead to Vanishing gradients and Mode collapse.\n",
        "\n",
        "WGAN addresses these issues by minimizing the **Wasserstein-1 distance (Earth Mover’s Distance)**:\n",
        "\n",
        "$$\n",
        "W(P_r, P_g) = \\inf_{\\gamma \\in \\Pi(P_r, P_g)} \\mathbb{E}_{(x, y) \\sim \\gamma} [\\|x - y\\|]\n",
        "$$\n",
        "\n",
        "This distance metric offers:\n",
        "\n",
        "✅ **More stable training processes**\n",
        "✅ **Enhanced gradient flow during training**\n",
        "✅ **Reduced susceptibility to mode collapse**\n",
        "\n",
        "---\n",
        "\n",
        "## **2️⃣ Enforcing the 1-Lipschitz Constraint**\n",
        "\n",
        "For WGAN methodology to be effective, the critic function $D(x)$ must satisfy the **1-Lipschitz condition**:\n",
        "\n",
        "$$\n",
        "| D(x_1) - D(x_2) | \\leq | x_1 - x_2 |\n",
        "$$\n",
        "\n",
        "The original WGAN implementation employed **weight clipping** to enforce this constraint. However, weight clipping has drawbacks:\n",
        "\n",
        "❌ Leads to suboptimal gradients\n",
        "❌ Reduces the learning capacity of the critic\n",
        "\n",
        "**WGAN-GP overcomes these limitations by introducing the Gradient Penalty (GP)**.\n",
        "\n",
        "---\n",
        "\n",
        "## **3️⃣ Gradient Penalty (GP)**\n",
        "\n",
        "Instead of directly constraining critic weights, **Gradient Penalty (GP) puts a penalty on the gradient norm** of the critic's output with respect to its input:\n",
        "\n",
        "$$\n",
        "\\lambda \\cdot (\\|\\nabla_{\\hat{x}} D(\\hat{x})\\|_2 - 1)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $ \\hat{x} $ represents an **interpolated sample**, created between a real data point $x_r$ and a generated data point $x_g$:\n",
        "\n",
        "  $$\n",
        "  \\hat{x} = \\alpha x_r + (1 - \\alpha) x_g, \\quad \\alpha \\sim U(0,1)\n",
        "  $$\n",
        "\n",
        "- $ \\lambda $ is the **gradient penalty coefficient**, typically set to **10**.\n",
        "\n",
        "- By penalizing deviations from a gradient norm of 1, **Lipschitz continuity is effectively enforced**.\n",
        "\n",
        "---\n",
        "\n",
        "## **4️⃣ WGAN-GP Loss Functions**\n",
        "\n",
        "###  **Critic (Discriminator) Loss**\n",
        "\n",
        "The critic's objective is to maximize the difference in scores between real and generated samples, while also incorporating the gradient penalty:\n",
        "\n",
        "$$\n",
        "L_D = \\mathbb{E}_{x_r \\sim P_r} [D(x_r)] - \\mathbb{E}_{x_g \\sim P_g} [D(x_g)] + \\lambda \\mathbb{E}_{\\hat{x} \\sim P_{\\hat{x}}} \\left[ (\\|\\nabla_{\\hat{x}} D(\\hat{x})\\|_2 - 1)^2 \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $ D(x_r) $ is the critic's output for **real data**.\n",
        "- $ D(x_g) $ is the critic's output for **generated data**.\n",
        "- The **gradient penalty term**\n",
        "\n",
        "### **Generator Loss**\n",
        "\n",
        "The generator aims to minimize the **negative critic scores** for generated samples:\n",
        "\n",
        "$$\n",
        "L_G = - \\mathbb{E}_{x_g \\sim P_g} [D(x_g)]\n",
        "$$\n",
        "\n",
        "Where $ x_g $ is the output from the generator network.\n",
        "\n",
        "**Note:** Sigmoid activation and Binary Cross-Entropy (BCE) loss are not used in WGAN-GP.\n",
        "\n",
        "---\n",
        "\n",
        "## **5️⃣ Training WGAN-GP**\n",
        "\n",
        "WGAN-GP training include:\n",
        "\n",
        "1. **Frequent Critic Updates:** Train the critic network more often than the generator (for example 5 critic updates for every generator update).\n",
        "2. **Adam Optimizer with Low Momentum:** Use the Adam optimizer with reduced momentum values (e.g., $β_1 = 0.0, β_2 = 0.9$).\n",
        "3. **Gradient Penalty Application:** Implement gradient penalty as the method for enforcing the Lipschitz constraint, replacing weight clipping.\n",
        "\n",
        "### **Pseudocode for Training**\n",
        "\n",
        "1. **Critic Training Steps:**\n",
        "   - Sample real data $x_r$ and random noise $z$.\n",
        "   - Generate fake data: $x_g = G(z)$.\n",
        "   - Calculate critic outputs: $D(x_r)$ and $D(x_g)$.\n",
        "   - Generate interpolated samples $\\hat{x}$.\n",
        "   - Compute the **gradient penalty** term.\n",
        "   - Calculate the **critic loss** $L_D$ and update critic network $D$.\n",
        "\n",
        "2. **Generator Training Steps:**\n",
        "   - Generate fake data: $x_g = G(z)$.\n",
        "   - Calculate generator loss $L_G = -D(x_g)$.\n",
        "   - Update generator network $G$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQVCOuTBSFLY"
      },
      "source": [
        "You can use the data using torch, Kaggle or via the follwoing link: [Dataset](https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8?resourcekey=0-5BR16BdXnb8hVj6CNHKzLg)\n",
        "\n",
        "you will need img_align_celeba.zip file, the label file is list_attr_celeba.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-28T19:53:36.137029Z",
          "iopub.status.busy": "2025-02-28T19:53:36.136738Z",
          "iopub.status.idle": "2025-02-28T19:54:56.685388Z",
          "shell.execute_reply": "2025-02-28T19:54:56.684503Z",
          "shell.execute_reply.started": "2025-02-28T19:53:36.137007Z"
        },
        "id": "mIQtI-UoJ6Vk",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import grad\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_ERA1p_acNtC"
      },
      "outputs": [],
      "source": [
        "batch_size = 64               # Number of images per batch\n",
        "image_size = 64               # Image dimensions (64x64)\n",
        "num_epochs = 50               # Number of training epochs\n",
        "learning_rate = 0.0001        # Learning rate for the optimizer\n",
        "beta1 = 0.0                   # Adam optimizer beta1 hyperparameter\n",
        "beta2 = 0.9                   # Adam optimizer beta2 hyperparameter\n",
        "latent_dim = 100              # Dimensionality of the latent vector (input to the generator)\n",
        "channels = 3                  # Number of color channels (3 for RGB)\n",
        "gradient_penalty_coefficient = 10  # Weight for the gradient penalty term\n",
        "critic_steps = 5              # Number of critic updates per generator update\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file and extraction folder\n",
        "zip_file_path = './img_align_celeba.zip'\n",
        "extracted_folder_path = './img_align_celeba'  # This is the folder where images will be extracted\n",
        "\n",
        "# Extract images from zip file if not already extracted\n",
        "if not os.path.exists(extracted_folder_path):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_folder_path)\n",
        "    print(\"Extraction completed.\")\n",
        "else:\n",
        "    print(\"Images are already extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kbz0MMycnJa"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Dataset and Dataloader\n",
        "# ---------------------------\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),         \n",
        "    transforms.CenterCrop(image_size),     \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset = dset.ImageFolder(root=extracted_folder_path, transform=transform)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kNjQjQKAczoK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=8192, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# (Optional) Weight Initialization Function\n",
        "# ---------------------------\n",
        "def weights_init(m):\n",
        "    \"\"\"\n",
        "    Custom weight initialization for Conv and BatchNorm layers.\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:  # Applies to both Conv2d and ConvTranspose2d\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)  # Normal distribution with mean 0 and std 0.02\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias.data, 0.0)  # Constant zero initialization for bias\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)  # Initialize weights with a normal distribution\n",
        "        nn.init.constant_(m.bias.data, 0.0)  # Bias initialized to zero\n",
        "\n",
        "# Example of applying weight initialization to generator and discriminator\n",
        "generator.apply(weights_init)\n",
        "discriminator.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7m7QFSkCcv9h"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ----------------------------------------------\n",
        "# Generator: Define your generator here, 20 Marks\n",
        "# ----------------------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, channels, image_size):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.init_size = image_size // 16 \n",
        "        self.l1 = nn.Linear(latent_dim, 128 * self.init_size * self.init_size)\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.l1(z)\n",
        "        out = out.view(out.size(0), 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "OqNLejgLdH6X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---------------------------\n",
        "# Discriminator (Critic): Define your discriminator here, 20 Marks\n",
        "# ---------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels, image_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, bn=True):\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n",
        "            if bn:\n",
        "                layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(channels, 64, bn=False), \n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(512 * (image_size // 16) ** 2, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        out = self.model(img)\n",
        "        out = out.view(out.size(0), -1) \n",
        "        validity = self.fc(out)\n",
        "        return validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MyBuTTSndH92"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import grad\n",
        "\n",
        "# ---------------------------\n",
        "# Gradient Penalty Function: Implement the gradient penalty calculation here, 30 Marks\n",
        "# ---------------------------\n",
        "def compute_gradient_penalty(D, real_samples, fake_samples, device):\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device, requires_grad=True)\n",
        "    alpha = alpha.expand_as(real_samples)\n",
        "\n",
        "    interpolates = alpha * real_samples + ((1 - alpha) * fake_samples)\n",
        "\n",
        "    interpolates = interpolates.to(device)\n",
        "    interpolates.requires_grad_(True)\n",
        "\n",
        "    d_interpolates = D(interpolates)\n",
        "\n",
        "    gradients = grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones(d_interpolates.size(), device=device),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "\n",
        "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-28T20:01:19.162637Z",
          "iopub.status.busy": "2025-02-28T20:01:19.162293Z"
        },
        "id": "M7P5W-K8J6Vl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Instantiate Models (Generator and Discriminator) & Initialize Weights\n",
        "# ---------------------------\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
        "        nn.init.normal_(m.weight.data, mean=0.0, std=0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "# Instantiate generator and discriminator\n",
        "generator = Generator(latent_dim=latent_dim, channels=channels, image_size=image_size).to(device)\n",
        "discriminator = Discriminator(channels=channels, image_size=image_size).to(device)\n",
        "\n",
        "# Initialize weights\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n",
        "\n",
        "# ---------------------------\n",
        "# Optimizers\n",
        "# ---------------------------\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
        "\n",
        "# ---------------------------\n",
        "# Fixed Noise for Visualization\n",
        "# ---------------------------\n",
        "fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)  # Create a fixed noise vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "execution_failed": "2025-02-28T20:45:58.359Z",
          "iopub.execute_input": "2025-02-28T20:01:19.162637Z",
          "iopub.status.busy": "2025-02-28T20:01:19.162293Z"
        },
        "id": "00KD-LieR7E0",
        "outputId": "2e906f7b-9f8e-4207-8f87-9946b2e434fc",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (6400x1 and 100x2048)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --------------------\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Train Discriminator (Critic)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# --------------------\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Generate fake images\u001b[39;00m\n\u001b[32m     16\u001b[39m z = torch.randn(batch_size, latent_dim, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m fake_images = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m.detach()  \u001b[38;5;66;03m# detach to avoid backprop through G\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Get discriminator outputs\u001b[39;00m\n\u001b[32m     20\u001b[39m real_validity = discriminator(real_images)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/test/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/test/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mGenerator.forward\u001b[39m\u001b[34m(self, z)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     out = out.view(out.size(\u001b[32m0\u001b[39m), \u001b[32m128\u001b[39m, \u001b[38;5;28mself\u001b[39m.init_size, \u001b[38;5;28mself\u001b[39m.init_size)\n\u001b[32m     37\u001b[39m     img = \u001b[38;5;28mself\u001b[39m.conv_blocks(out)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/test/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/test/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/test/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (6400x1 and 100x2048)"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# WGAN-GP Training Loop: Implement the training loop here, 30 Marks\n",
        "# ---------------------------\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, _) in enumerate(dataloader):\n",
        "        # Move data to the appropriate device\n",
        "        real_images = real_images.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # --------------------\n",
        "        # Train Discriminator (Critic)\n",
        "        # --------------------\n",
        "\n",
        "        # Generate fake images\n",
        "        z = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "        fake_images = generator(z).detach()  # detach to avoid backprop through G\n",
        "\n",
        "        # Get discriminator outputs\n",
        "        real_validity = discriminator(real_images)\n",
        "        fake_validity = discriminator(fake_images)\n",
        "\n",
        "        # Compute Gradient Penalty\n",
        "        gradient_penalty = compute_gradient_penalty(discriminator, real_images, fake_images, device)\n",
        "\n",
        "        # Calculate Discriminator (Critic) loss (including GP)\n",
        "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + gradient_penalty_coefficient * gradient_penalty\n",
        "\n",
        "        # Backward and optimize critic\n",
        "        optimizer_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator every `critic_steps`\n",
        "        if i % critic_steps == 0:\n",
        "            # --------------------\n",
        "            # Train Generator\n",
        "            # --------------------\n",
        "\n",
        "            # Generate fake images\n",
        "            z = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "            fake_images = generator(z)\n",
        "\n",
        "            # Calculate Generator loss (negative critic output for fake images)\n",
        "            g_loss = -torch.mean(discriminator(fake_images))\n",
        "\n",
        "            # Backward and optimize generator\n",
        "            optimizer_G.zero_grad()\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        # Optional: Print training progress\n",
        "        if i % 50 == 0:\n",
        "            print(f\"[Epoch {epoch+1}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n",
        "                  f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
        "\n",
        "    # Save/Visualize generated images (optional, e.g., every epoch)\n",
        "    with torch.no_grad():\n",
        "        fake_images = generator(fixed_noise).cpu()\n",
        "    grid = vutils.make_grid(fake_images, padding=2, normalize=True)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Epoch {epoch+1}\")\n",
        "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 29561,
          "sourceId": 37705,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
