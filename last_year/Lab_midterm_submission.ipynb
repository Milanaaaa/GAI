{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1ECgH4CAKbj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Midterm Lab Exam\n",
    "\n",
    "```\n",
    "- Generative Artificial Intelligence (Fall semester 2023)\n",
    "- Professor: Muhammad Fahim\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "## Tasks\n",
    "```\n",
    "1. Variational AutoEncoder\n",
    "2. Conditional GAN (bonus)\n",
    "```\n",
    "All implementation in python and using PyTorch framework for neural networks\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NQf-CxiAKbl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Dataset\n",
    "\n",
    "For both tasks CIFAR-10 dataset will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfPgcc23AKbm",
    "outputId": "ba9cc60b-92cd-4a41-dc08-06a63cacb73d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "available device : cuda\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'available device : {device}')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouN4P45sAKbn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Task 1 (variational autoencoder)\n",
    "\n",
    "* Implement and train a variational autoencoder for cifar 10 data using the achitecture below for generator and discriminator as baseline (you can only improve the achitecture by extending the achitecture on top of baseline)\n",
    "* Implement a function that will generate images using the implemented vae\n",
    "\n",
    "\n",
    "### 2.1  VAE definition (20 points)\n",
    "\n",
    "**Encoder Achitecture (baseline)**\n",
    "- 3 convolutional layers whereby each layer is followed by batch normalization and relu activation function\n",
    "  - Layer 1 : applies 32 filters\n",
    "  - Layer 2 : applies 64 filters\n",
    "  - Layer 3 : applies 128 filters\n",
    "\n",
    "**Latent space**\n",
    "- without activation\n",
    "\n",
    "**Decoder Achitecture (baseline)**\n",
    "- 3 deconvolutional layers whereby each layer is followed by batch normalization and relu activation function\n",
    "- Mirror of the encoder network\n",
    "\n",
    "**NB**: For both convolution and deconvolution `kernel_size=4, stride=2, padding=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Vx6hVH2AKbn",
    "outputId": "a02e69b9-87c0-43a2-a865-1723671311e9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (mean): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (variance): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, channel_num=3, kernel_num=128, z_size=2048, image_size=32):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Encoder architecture:\n",
    "        3 convolutional layers whereby each layer is followed by batch normalization and relu activation function\n",
    "            Layer 1 : applies 32 filters\n",
    "            Layer 2 : applies 64 filters\n",
    "            Layer 3 : applies 128 filters\n",
    "        \"\"\"\n",
    "        # set up the parameters\n",
    "        n_filters_list = [32, 64, 128]\n",
    "        kernel_size = 4\n",
    "        stride = 2\n",
    "        padding = 1\n",
    "        self.z_size = z_size   # the length of the mean and variance vectors\n",
    "\n",
    "        def build_conv_layer(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        ):\n",
    "            return nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding\n",
    "            )\n",
    "\n",
    "        def build_deconv_layer(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        ):\n",
    "            return nn.ConvTranspose2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding\n",
    "            )\n",
    "\n",
    "        # build an encoder Sequential object\n",
    "        self.encoder = nn.Sequential(\n",
    "            build_conv_layer(in_channels=channel_num, out_channels=n_filters_list[0]),\n",
    "            nn.BatchNorm2d(n_filters_list[0]),\n",
    "            nn.ReLU(),\n",
    "            build_conv_layer(in_channels=n_filters_list[0], out_channels=n_filters_list[1]),\n",
    "            nn.BatchNorm2d(n_filters_list[1]),\n",
    "            nn.ReLU(),\n",
    "            build_conv_layer(in_channels=n_filters_list[1], out_channels=n_filters_list[2]),\n",
    "            nn.BatchNorm2d(n_filters_list[2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # latent representation layers that produce mean and variance vectors\n",
    "        self.mean = nn.Linear(z_size, z_size)\n",
    "        self.variance = nn.Linear(z_size, z_size)\n",
    "\n",
    "        \"\"\"\n",
    "        Decoder architecture:\n",
    "        3 deconvolutional layers whereby each layer is followed by batch normalization and relu activation function (a mirror of the encoder network)\n",
    "        \"\"\"\n",
    "        self.decoder = nn.Sequential(\n",
    "            build_deconv_layer(in_channels=n_filters_list[2], out_channels=n_filters_list[1]),\n",
    "            nn.BatchNorm2d(n_filters_list[1]),\n",
    "            nn.ReLU(),\n",
    "            build_deconv_layer(in_channels=n_filters_list[1], out_channels=n_filters_list[0]),\n",
    "            nn.BatchNorm2d(n_filters_list[0]),\n",
    "            nn.ReLU(),\n",
    "            build_deconv_layer(in_channels=n_filters_list[0], out_channels=channel_num),\n",
    "            nn.BatchNorm2d(channel_num),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def reparametrize(self, mean, variance):\n",
    "        std = torch.exp(variance / 2)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + epsilon * std\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Your code here\n",
    "        latent = self.encoder(x)\n",
    "        latent_flatten = latent.view(x.size(0), -1) # flattening\n",
    "        # print(latent.shape)\n",
    "        # print(latent_flatten.shape)\n",
    "        # print(latent_flatten)\n",
    "\n",
    "        mean, logvar = self.mean(latent_flatten), self.variance(latent_flatten)\n",
    "\n",
    "        r_flatten = self.reparametrize(mean, logvar)    # reparameterization\n",
    "        r = torch.reshape(r_flatten, latent.shape)  # unflattening\n",
    "        x_reconstructed = self.decoder(r)\n",
    "\n",
    "        return (mean, logvar), x_reconstructed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vae_model = VAE()\n",
    "vae_model"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for data, _ in trainloader:\n",
    "    latent_shape = vae_model.encoder(data).shape\n",
    "    break\n",
    "latent_shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkVWbuHFR834",
    "outputId": "76e46fc2-b7f1-41aa-d759-ab0ee14997f9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 4, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4WsIwYBAKbn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Training parameters definition (10 points)\n",
    "\n",
    "- Define optimizer : Adam optimizer (default `weight_decay=1e-5, learning_rate=3e-04`)\n",
    "- Define the criterion : kl-divergence loss and reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FNeqouKNAKbn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae_model.parameters(), weight_decay=1e-5, lr=3e-04)\n",
    "\n",
    "def reconstruction_loss(x_reconstructed, x):\n",
    "    ## Your code here\n",
    "    return F.mse_loss(x, x_reconstructed, reduction='sum')  # we're summing the pixel-wise loss\n",
    "\n",
    "def kl_divergence_loss(mean, logvar):\n",
    "    ## Your code here\n",
    "    return -torch.sum(1 + logvar - mean ** 2 - logvar.exp()) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfnJ1-AgAKbo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 VAE Training and Evaluation (15 points)\n",
    "\n",
    "- Define the training procedure and train the vae model\n",
    "- Add model evaluation every after (n batches) or (n epochs)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10\n",
    "vae_model.train()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    reconstruction_loss_sum, kl_divergence_loss_sum = 0, 0\n",
    "\n",
    "    for data, _ in trainloader:\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        (mean, logvar), x_reconstructed = vae_model(data)\n",
    "        # compute the losses\n",
    "        reconstruction_loss_batch = reconstruction_loss(x_reconstructed, data)\n",
    "        kl_divergence_loss_batch = kl_divergence_loss(mean, logvar)\n",
    "        loss_batch = reconstruction_loss_batch + kl_divergence_loss_batch\n",
    "        # backprop\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        # sum the losses\n",
    "        reconstruction_loss_sum += reconstruction_loss_batch.item()\n",
    "        kl_divergence_loss_sum += kl_divergence_loss_batch.item()\n",
    "\n",
    "    # print losses per epoch\n",
    "    print(f'Epoch {epoch}:')\n",
    "    print(f'Reconstruction loss: {reconstruction_loss_sum}')\n",
    "    print(f'KL divergence loss: {kl_divergence_loss_sum}')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ku-ULTCJBIt4",
    "outputId": "a64a5dd9-6894-4e90-c774-5d21020bf0a8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 10%|█         | 1/10 [00:58<08:48, 58.74s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0:\n",
      "Reconstruction loss: 6331143.8525390625\n",
      "KL divergence loss: 469905.4650878906\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 20%|██        | 2/10 [01:54<07:37, 57.20s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1:\n",
      "Reconstruction loss: 6083635.541015625\n",
      "KL divergence loss: 363860.82287597656\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 30%|███       | 3/10 [02:41<06:06, 52.31s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2:\n",
      "Reconstruction loss: 6011450.5732421875\n",
      "KL divergence loss: 347919.7263183594\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 40%|████      | 4/10 [03:29<05:04, 50.71s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3:\n",
      "Reconstruction loss: 5968876.3818359375\n",
      "KL divergence loss: 339364.62127685547\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 50%|█████     | 5/10 [04:18<04:10, 50.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4:\n",
      "Reconstruction loss: 5937293.4833984375\n",
      "KL divergence loss: 331096.41525268555\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 60%|██████    | 6/10 [05:07<03:18, 49.67s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5:\n",
      "Reconstruction loss: 5906473.2529296875\n",
      "KL divergence loss: 331338.4747314453\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 70%|███████   | 7/10 [05:54<02:26, 48.69s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6:\n",
      "Reconstruction loss: 5841626.037109375\n",
      "KL divergence loss: 322981.98828125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 80%|████████  | 8/10 [06:41<01:36, 48.18s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7:\n",
      "Reconstruction loss: 5686894.275390625\n",
      "KL divergence loss: 320126.23767089844\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 90%|█████████ | 9/10 [07:28<00:47, 47.76s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8:\n",
      "Reconstruction loss: 5593394.751953125\n",
      "KL divergence loss: 321101.8046875\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [08:14<00:00, 49.43s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9:\n",
      "Reconstruction loss: 5565116.6240234375\n",
      "KL divergence loss: 322868.0987548828\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx8vf4okAKbo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 Images generation (5 points)\n",
    "\n",
    "- Implement a function that will take the number of images to be generated, generate images using the previously implemented vae and save them to folder"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_images(num_images=5, model=vae_model):\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.makedirs(\"./output\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_images):\n",
    "            n = torch.randn(1, 2048)    # sample ~N(0, 1)\n",
    "\n",
    "            n = torch.reshape(n, [1, 128, 4, 4])  # unflattening\n",
    "            # n = n.to(device)  # Send z to the same device as the model\n",
    "\n",
    "            generation_img = model.decoder(n)\n",
    "\n",
    "            save_image(generation_img.view(1, 3, 32, 32), f'./output/img{i}.png')"
   ],
   "metadata": {
    "id": "1_t0Dm7-BRqN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generate_images()"
   ],
   "metadata": {
    "id": "i7qCzABtQ9j3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlrpN-F4AKbp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bonus task (50 points)\n",
    "\n",
    "- Grading for the bonus task is binary (its eighter you get it all correct or zero) -- One mistake equals zero\n",
    "- The bonus points scope is the midterm lab exam only (bonus points cannot be tranfered to other parts of the course)\n",
    "- Implement a conditional GAN for CIFAR 10 data\n",
    "- Train and evaluate cGAN\n",
    "- Log the training and validation perfomace metrics to `TensorBoard`\n",
    "- Implement a function that will take a condition, generate images using the previously implemented conditional GAN and visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-JNABg-AKbp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QubxgqUzAKbp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}