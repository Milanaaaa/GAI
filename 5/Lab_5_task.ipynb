{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73JHBwym-IfY"
      },
      "source": [
        "## Week 5 : Generative AI for Language Models (Recurrent neural networks, LSTM)\n",
        "```\n",
        "- Generative Artificial Intelligence (Spring semester 2025)\n",
        "- Professor: Muhammad Fahim\n",
        "- Teaching Assistant: Ahmad Taha\n",
        "```\n",
        "<hr>\n",
        "\n",
        "## Contents\n",
        "```\n",
        "Lab Plan\n",
        "1. Dataset (SenseEval)\n",
        "2. Data Preprocessing\n",
        "3. Recurrent neural networks (PyTorch RNN)\n",
        "4. Long short-term memory (PyTorch LSTM)\n",
        "5. Self practice task\n",
        "```\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ix2fmCL_N6-"
      },
      "source": [
        "## Recap\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchtext==0.15.2"
      ],
      "metadata": {
        "id": "9XCgJBt0sGlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VThX0jkS9_0D",
        "outputId": "3aaa7385-2789-4e39-a27b-59fdb2528bce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Preliminaries for processing the text\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "import torchtext\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGSqqYCE9poX"
      },
      "source": [
        "## Basics (Sequences and RNN)\n",
        "\n",
        "\n",
        "Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state. The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors\n",
        "\n",
        "![](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "### Mode details on the RNN cell\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment7.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ7f72pc9soF"
      },
      "outputs": [],
      "source": [
        "simple_sequence = torch.Tensor([[0.3,1.9,4.5],[0.4,0.1,0.23],[0.7,0.91,0.43], [0.34,0.01,0.002]])\n",
        "\n",
        "simple_sequence = simple_sequence.unsqueeze(0)\n",
        "\n",
        "simple_sequence.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6sGNZV_Ahev"
      },
      "source": [
        "The `simple_sequence` variable represents a sequence of length 4, where each element (time-stamp) is represented by a feature vector of length 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG0rJymoAum9"
      },
      "source": [
        "## Basic RNN layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Fg1s20W5AtO9"
      },
      "outputs": [],
      "source": [
        "simple_rnn_layer = nn.RNN(input_size=3, hidden_size=2, num_layers = 2, bias = True, batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLGIlMmzAgwU"
      },
      "outputs": [],
      "source": [
        "simple_rnn_layer.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GHIgqbFnCbXz"
      },
      "outputs": [],
      "source": [
        "output_all, output_last = simple_rnn_layer(simple_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywk8pq3DC3l3"
      },
      "outputs": [],
      "source": [
        "output_last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B8x_dSpDw3J"
      },
      "outputs": [],
      "source": [
        "output_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXw0izpPIdP2"
      },
      "source": [
        "### Inside the RNN cell\n",
        "\n",
        "$$a^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}$$\n",
        "$$h^{(t)} = tanh(a^{(t)})$$\n",
        "\n",
        "where the parameters are the bias vectors `b` and `c` along with the weight matrices\n",
        "`U, V and W`, respectively for input-to-hidden, hidden-to-output and hidden-tohidden connections. <br>\n",
        "Lets see whats inside Pytorch and compare with our theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMEkOXUeuSCx"
      },
      "source": [
        "### Inside the RNN cell (1)\n",
        "\n",
        "$$h_t = tanh(W_{hi}x_t + b_{ih} + W_{hh}h_{(t-1)} + b_{hh})$$\n",
        "\n",
        "where $h_t$ represents the hidden state at time $t$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7hrvKzdvBIG"
      },
      "outputs": [],
      "source": [
        "wih = simple_rnn_layer.weight_ih_l0.squeeze(0)\n",
        "whh = simple_rnn_layer.weight_hh_l0.squeeze(0)\n",
        "\n",
        "bih = simple_rnn_layer.bias_ih_l0\n",
        "bhh = simple_rnn_layer.bias_hh_l0\n",
        "\n",
        "x = simple_sequence[0][0] # The first input feature of the first sequence\n",
        "\n",
        "# Computing thw hidden state for time = 1\n",
        "h1 = torch.tanh(torch.Tensor(torch.dot(x,wih) + bih  + torch.dot(whh,torch.Tensor([0.0])) + bhh))\n",
        "h1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGrxHjry0BWl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l52dYDXuz5Rj"
      },
      "outputs": [],
      "source": [
        "x = simple_sequence[0][1]\n",
        "\n",
        "h2 = torch.tanh(torch.Tensor(torch.dot(x, wih) + bih  + torch.dot(whh,h1) + bhh))\n",
        "h2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J_tOEYlMS1_"
      },
      "source": [
        "**Task** : Compute all the other hidden states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59rflO2pMefK"
      },
      "outputs": [],
      "source": [
        "result = []\n",
        "\n",
        "h_previous = torch.Tensor([0.0])\n",
        "\n",
        "for i in range(simple_sequence.shape[1]):\n",
        "  # TODO: Compute and print the hidden states using the given example\n",
        "\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mTHqb7vC6Pb"
      },
      "outputs": [],
      "source": [
        "# !pip install -U torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTbmX_Gu_dJx"
      },
      "source": [
        "## 1. Dataset and Problem statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9gx5dyI_dl5"
      },
      "outputs": [],
      "source": [
        "## Downloading Dataset\n",
        "\n",
        "!pip install wget\n",
        "import wget\n",
        "\n",
        "#Download and unzip dataset\n",
        "wget.download(\"http://alt.qcri.org/semeval2016/task6/data/uploads/stancedataset.zip\")\n",
        "\n",
        "!unzip stancedataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR_TlPCFAXHU"
      },
      "source": [
        "### 1.2 Read dataset to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RfcB6BEuAa7d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(\"./StanceDataset/train.csv\", header=0, engine='python' ,encoding = \"latin-1\", usecols=[\"Tweet\",\"Target\"])\n",
        "test_data = pd.read_csv(\"./StanceDataset/test.csv\", header=0, engine='python' ,encoding = \"latin-1\", usecols=[\"Tweet\",\"Target\"])\n",
        "\n",
        "test_data.query(\"Target != 'Donald Trump'\", inplace=True)\n",
        "\n",
        "labels_keys = {value: i for i, (value, count) in enumerate(train_data.Target.value_counts().items())}\n",
        "\n",
        "train_data['Target'] = train_data['Target'].apply(lambda x: labels_keys.get(x))\n",
        "test_data['Target'] = test_data['Target'].apply(lambda x: labels_keys.get(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp1lWAIyKnwu"
      },
      "outputs": [],
      "source": [
        "np.unique(labels_keys.values())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "vu2V-jC30ZLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irqY1J9L_YSR"
      },
      "source": [
        "## 2.Data Preprocessing\n",
        "\n",
        "[`torchtext`](https://pytorch.org/text/stable/index.html) is a package that consists of data processing utilities and popular datasets for natural language\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKiydYFJAmuv"
      },
      "source": [
        "### 1.3 Clean, tokenize and create Vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3vsiebp_AyT2"
      },
      "outputs": [],
      "source": [
        "def clean_ascii(text):\n",
        "  #remove non-ASCII chars from data\n",
        "  return ''.join(i for i in text if ord(i) < 128)\n",
        "\n",
        "train_data['Tweet'] = train_data['Tweet'].apply(clean_ascii)\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "counter = Counter()\n",
        "\n",
        "for _, row in train_data.iterrows():\n",
        "  counter.update(tokenizer(row[\"Tweet\"]))\n",
        "\n",
        "\n",
        "vocablary = vocab(counter, specials=(\"<pad>\",\"<unk>\"), min_freq=1)\n",
        "vocablary.set_default_index(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH3FpGZ_ug2G"
      },
      "outputs": [],
      "source": [
        "list(counter.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yBN8wJgbuH5A"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, OrderedDict\n",
        "unk_token = '<unk>'\n",
        "default_index = -1\n",
        "v2 = vocab(OrderedDict([(token, 1) for token in list(counter.keys())]), specials=[unk_token])\n",
        "v2.set_default_index(default_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E64fNpzDA9eB"
      },
      "source": [
        "### 1.4 Padding Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PcNrk5nPBEt5"
      },
      "outputs": [],
      "source": [
        "# Do padding\n",
        "def data_process(raw_text_iter,max_len=64):\n",
        "  batch = []\n",
        "  for item in raw_text_iter:\n",
        "    res = [v2[token] for token in tokenizer(item)]\n",
        "    if len(res) > max_len :\n",
        "      res = res[:max_len]\n",
        "    if len(res) < max_len :\n",
        "      res += ([v2[\"<pad>\"]] * (max_len-len(res)))\n",
        "    batch.append(res)\n",
        "  pad_data = torch.tensor(batch, dtype=torch.long)\n",
        "  return pad_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdUgotn0BbcC"
      },
      "source": [
        "## Create Dataloaders\n",
        "\n",
        "\n",
        "<font color='red'>**Task**</font>: Create validation dataloader from train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh6y1XX0_b7K"
      },
      "outputs": [],
      "source": [
        "max_len = 64\n",
        "embedding_size = 10\n",
        "n_classes = len(np.unique(train_data.Target.values))\n",
        "\n",
        "#Create Dataloader\n",
        "train_tensor = data_process(train_data.Tweet.values)\n",
        "tgts_tensor = torch.from_numpy(train_data.Target.values)\n",
        "\n",
        "# Test\n",
        "test_tensor = data_process(test_data.Tweet.values)\n",
        "test_labels = torch.from_numpy(test_data.Target.values)\n",
        "\n",
        "# Valid\n",
        "# TODO : Create validation\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(train_tensor, tgts_tensor)\n",
        "test_dataset = TensorDataset(test_tensor, test_labels)\n",
        "\n",
        "train_iterator = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
        "test_iterator = DataLoader(test_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
        "valid_iterator = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpjyrjUXNVBS"
      },
      "source": [
        "## Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04oPle1jNSzU"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    # TODO: Define the RNN layer and fully connected layer\n",
        "\n",
        "    self.embedding_layer = nn.Embedding(input_dim, embedding_dim)\n",
        "    self.rnn_cell = nn.RNN(...)\n",
        "    self.fc_layer = nn.Linear(...)\n",
        "\n",
        "  def forward(self, text):\n",
        "    \"\"\"\n",
        "    Foward pass method\n",
        "    \"\"\"\n",
        "    # TODO: Define the forward method\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwvso1QU4vt1"
      },
      "source": [
        "### Training Model\n",
        "\n",
        "<font color='red'>**Task**</font>: Define a function for calculating accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js1liVNffvTn"
      },
      "outputs": [],
      "source": [
        "def accuracy_calculator(preds, y):\n",
        "  \"\"\"Returns accuracy per batch\"\"\"\n",
        "  # TODO implement a function that returns accuracy per batch\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnXH-OUcojrM"
      },
      "source": [
        "\n",
        "\n",
        "<font color='red'>**Task**</font>: Define a training loop for training the RNN model<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roKuxTXW4vL7"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch in dataloader:\n",
        "    # TODO: Define a training loop for training the RNN model\n",
        "    optimizer.zero_grad()\n",
        "    predictions = None\n",
        "    loss = None\n",
        "\n",
        "    acc = accuracy_calculator(predictions, batch.label)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "\n",
        "  return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zDj1sBHogEh"
      },
      "source": [
        "<font color='red'>**Task**</font>: Define a function for evaluating the model on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvWCA7YC4uEe"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_batches, criterion):\n",
        "  eval_loss = 0\n",
        "  eval_acc = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_batches:\n",
        "      # TODO : Define a function for evaluating the model on test set\n",
        "      predictions = None\n",
        "      loss = None\n",
        "\n",
        "      acc = accuracy_calculator(None, None)\n",
        "      eval_loss += loss.item()\n",
        "      eval_acc += acc.item()\n",
        "\n",
        "  return eval_loss / len(data_batches), eval_acc / len(data_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOQ5WxteE5fo"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfHRImuoE8GV"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "input_dim = len(vocablary) #input dimension is the dimension of the one-hot vectors\n",
        "embedding_dim = 100\n",
        "hidden_dim = 10 #size of the hidden states\n",
        "output_dim = 1\n",
        "\n",
        "# TODO: create the RNN model\n",
        "model = None\n",
        "\n",
        "# define loss function and optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = None\n",
        "\n",
        "#make model instance and send it to training device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYTuVZotGVIu"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaBj11amrXXh"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O6x3Df06i6t"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate_model(model, valid_iterator, criterion)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), 'best-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1} , Train [Loss:  {train_loss:.3f}  Acc :{train_acc*100:.2f}], Val.[Loss: {valid_loss:.3f} Acc: {valid_acc*100:.2f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM9_fs8d7Djk"
      },
      "source": [
        "### Load best model for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXfK85vt7HzD"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best-model.pt')) #Load the best model\n",
        "test_loss, test_acc = evaluate_model(model, test_iterator, criterion)\n",
        "print(f'Accuracy on test data : {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o8n7Ht9BivV"
      },
      "source": [
        "## How to check for vanishing/exploding gradients ?\n",
        "\n",
        "* This is a problem that involves weights in earlier layers of the network. Why? (hint : stochastic gradient descent)\n",
        "* The vanishing gradient problem is a problem that causes major difficulty when training a neural network.\n",
        "* If the gradient is vanishingly small, then the weights update during backpropergation are going to be vanishingly small as well.\n",
        "\n",
        "\n",
        "**How to detect this?**\n",
        "1. Monitor the weights i.e use TensorBoard and log the weights\n",
        "2. Make checkpoints and manualy log to track :\n",
        "```\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.grad.norm())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3i9vVk0NTMP"
      },
      "source": [
        "# Long short-term memory (LSTM)\n",
        "\n",
        "![](https://www.researchgate.net/publication/329362532/figure/fig5/AS:699592479870977@1543807253596/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DICQLXyDa4yu"
      },
      "source": [
        "## Basics of LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOBCKWGBa4VW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3d-Zsqoa-uz"
      },
      "source": [
        "<font color='red'>**Task**</font>: Using\n",
        "The SemEval-2016 Stance Dataset. train a LSTM model for sentiment analysis. In the dataset there is a column **`sentiment`**, use it as target. To make the task binary classification remove samples in the dataset with label **`other`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Clb1EJpjb0K-"
      },
      "outputs": [],
      "source": [
        "# TODO: Data preprocessing and creation of dataloaders (train, validation & test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKtmFY2Gtb0k"
      },
      "source": [
        "## Define Simple LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKrYR85uMbwO"
      },
      "outputs": [],
      "source": [
        "class SimpleLstm(nn.Module):\n",
        "  def __init__(self, embedding_dim ,vocab_size , hidden_dim=10, output_dim=1, n_layers=1):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm_layer = nn.LSTM(...)\n",
        "\n",
        "    self.output_layer = nn.Linear(...)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    embedded = self.embedding(x)\n",
        "    outputs, (hidden, cell) = self.lstm_layer(embedded)\n",
        "\n",
        "    # TODO :\n",
        "    pred = self.output_layer(...)\n",
        "    return pred\n",
        "\n",
        "vocab_size = len(vocablary)\n",
        "embedding_size = 64\n",
        "output_dim = None # TODO\n",
        "model = SimpleLstm(embedding_dim=embedding_size, vocab_size=vocab_size, hidden_dim=10,output_dim=output_dim).to(device).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9YUoTK_tgps"
      },
      "source": [
        "## Training and Evaluating LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cktrfvhYcFY7"
      },
      "outputs": [],
      "source": [
        "# TODO : You will need to redefine accuracy_calculator function\n",
        "def accuracy_calculator(pred, y):\n",
        "  return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaxNfSeRMrpV"
      },
      "outputs": [],
      "source": [
        "# Train loop\n",
        "criterion = None\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "criterion = criterion.to(device)\n",
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate_model(model, valid_iterator, criterion)\n",
        "\n",
        "  print(f'Epoch: {epoch+1} , Train [Loss:  {train_loss:.3f}  Acc :{train_acc*100:.2f}], Val.[Loss: {valid_loss:.3f} Acc: {valid_acc*100:.2f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EukNgkJzsV_L"
      },
      "source": [
        "## <center>Self practice Task</center>\n",
        "\n",
        "```\n",
        "1. From Senseval data RNN and LSTM model to do prediction of the tags on ambiguous words\n",
        "2. Use pretrained embeddings instead of training from scratch with pretrained embeddings (glove or fasttext)\n",
        "```\n",
        "**To download Senseval data example**\n",
        "```\n",
        "from nltk.corpus import senseval as se\n",
        "nltk.download('senseval')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2O-Tb8ktRDC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lG0rJymoAum9",
        "WOQ5WxteE5fo",
        "HaBj11amrXXh",
        "M9YUoTK_tgps",
        "EukNgkJzsV_L"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}