{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVWoX5adp7bx",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Week 4 : Generative Adversarial Networks\n",
        "```\n",
        "- Generative Artificial Intelligence (Spring semester 2025)\n",
        "- Professor: Muhammad Fahim\n",
        "- Teaching Assistant: Ahmad Taha\n",
        "```\n",
        "<hr>\n",
        "\n",
        "\n",
        "```\n",
        "Lab Plan\n",
        "    1. Vanila GAN achitecture\n",
        "    2. GAN training procedure\n",
        "    3. Conditional GAN\n",
        "```\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yMz7v3Yp7cA",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 1. Vannila Generative adversarial network (GAN)\n",
        "\n",
        "![caption](https://www.researchgate.net/profile/Zhaoqing-Pan/publication/331756737/figure/fig1/AS:736526694621184@1552613056409/The-architecture-of-generative-adversarial-networks.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lE55zaEp7cA",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### 1.1 Dataset\n",
        "\n",
        "For this lesson we will use SVHN dataset which readily available in `torchvision` and we will do minimal transformation operations\n",
        "\n",
        "Install `torchvision` : `pip install torchvision`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgVjuBIo9YcY",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d071278-24ff-4bac-aee6-3449459bb9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 182M/182M [00:34<00:00, 5.28MB/s]\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize([32, 32]),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "# SVHN training datasets\n",
        "svhn_train = datasets.SVHN(root='data/', split='train', download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "num_workers = 0\n",
        "\n",
        "# build DataLoaders for SVHN dataset\n",
        "train_loader = torch.utils.data.DataLoader(dataset=svhn_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwRO4sTip7cC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 1.2 Generator & Discriminator Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyAK6ydmp7cC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#ngf : Number of generator filters\n",
        "#ndf : Number of discriminator filters\n",
        "nz = 32\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, ndf=3, conv_dim=32):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, 1, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(5*5,1)\n",
        "          )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Step 1: pass the input (real or fake samples) through all hidden layers\n",
        "        return self.model(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_size, ngf, conv_dim=32):\n",
        "        super(Generator, self).__init__()\n",
        "        # Step 1: Define the generator network architecture\n",
        "        # NOTE: the input is the random noise size and output is conv_dim i.e (3,32,32)\n",
        "        self.conv_dim = conv_dim\n",
        "        self.input_layer = nn.Linear(in_features=z_size, out_features=2048, bias=True)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels = 128, out_channels=ngf * 2, kernel_size=4,stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features= ngf * 2),\n",
        "            nn.Tanh(),\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=ngf),\n",
        "            nn.Tanh(),\n",
        "            nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Step 1: pass the input which is random noise to generate the face samples\n",
        "      # x = self.input_layer(x)\n",
        "      # x = x.view(-1, self.conv_dim*4, 4, 4) # (batch_size, depth, 4, 4)\n",
        "      return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epToMkpwp7cD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 1.3 Set hyperparams and training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RgJwvYBp7cD",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53f9a5e-9e91-43b0-a0f1-91878c9acf1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(64, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): Flatten(start_dim=1, end_dim=-1)\n",
            "    (8): Linear(in_features=25, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Generator(\n",
            "  (input_layer): Linear(in_features=100, out_features=2048, bias=True)\n",
            "  (model): Sequential(\n",
            "    (0): ConvTranspose2d(128, 6, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): Tanh()\n",
            "    (3): ConvTranspose2d(6, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): Tanh()\n",
            "    (6): ConvTranspose2d(3, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# define hyperparams\n",
        "conv_dim = 32\n",
        "z_size = 100\n",
        "num_epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# define discriminator and generator\n",
        "D = Discriminator(conv_dim).to(device)\n",
        "G = Generator(z_size=z_size, ngf=3,conv_dim=conv_dim).to(device)\n",
        "\n",
        "#print the models summary\n",
        "print(D)\n",
        "print()\n",
        "print(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHnvFXdzp7cE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 1.4 Define the loss function for D(x) and G(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6KcJQGIp7cE",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def real_loss(D_out, smooth=False):\n",
        "    batch_size = D_out.size(0)\n",
        "    # label smoothing\n",
        "    if smooth:\n",
        "        # smooth, real labels\n",
        "        labels = torch.FloatTensor(batch_size).uniform_(0.9, 1).to(device)\n",
        "    else:\n",
        "        labels = torch.ones(batch_size) # real labels = 1\n",
        "    # move labels to GPU if available\n",
        "\n",
        "    labels = labels.to(device)\n",
        "    # binary cross entropy with logits loss\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculate loss\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n",
        "def fake_loss(D_out):\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.FloatTensor(batch_size).uniform_(0, 0.1).to(device) # fake labels = 0\n",
        "    labels = labels.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculate loss\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n",
        "# params\n",
        "learning_rate = 0.0003\n",
        "beta1=0.5\n",
        "beta2=0.999 # default value\n",
        "\n",
        "# Create optimizers for the discriminator and generator\n",
        "d_optimizer = optim.Adam(D.parameters(), learning_rate)\n",
        "g_optimizer = optim.SGD(G.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkWInd2sp7cG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 2. GAN training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tQDq36wp7cG",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "e6abee26-61e6-4627-acc1-7f8006f6ce93"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given transposed=1, weight of size [128, 6, 4, 4], expected input[256, 100, 1, 1] to have 128 channels, but got 100 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3bc064a39de8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1035e28e1eff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;31m# x = self.input_layer(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;31m# x = x.view(-1, self.conv_dim*4, 4, 4) # (batch_size, depth, 4, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [128, 6, 4, 4], expected input[256, 100, 1, 1] to have 128 channels, but got 100 channels instead"
          ]
        }
      ],
      "source": [
        "# Logging\n",
        "print_every = 2\n",
        "\n",
        "# Get some fixed data for sampling. These are images that are held\n",
        "# constant throughout training, and allow us to inspect the model's performance\n",
        "sample_size=16\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\n",
        "\n",
        "# train the network\n",
        "for epoch in range(num_epochs):\n",
        "  g_l = 0\n",
        "  d_l = 0\n",
        "  for batch_i, (real_images, _) in enumerate(train_loader):\n",
        "\n",
        "    batch_size = real_images.size(0)\n",
        "\n",
        "\n",
        "    # TRAIN THE DISCRIMINATOR\n",
        "    # Step 1: Zero gradients (zero_grad)\n",
        "    # Step 2: Train with real images\n",
        "    # Step 3: Compute the discriminator losses on real images\n",
        "    d_optimizer.zero_grad()\n",
        "    real_images = real_images.to(device)\n",
        "    D_real = D(real_images)\n",
        "    d_real_loss = real_loss(D_real)\n",
        "\n",
        "    # Step 4: Train with fake images\n",
        "    # Step 5: Generate fake images and move x to GPU, if available\n",
        "    # Step 6: Compute the discriminator losses on fake images\n",
        "    # Step 7: add up loss and perform backprop\n",
        "\n",
        "    z = torch.FloatTensor(batch_size, z_size, 1, 1).uniform_(-1, 1).to(device)\n",
        "    fake_images = G(z)\n",
        "\n",
        "    D_fake = D(fake_images)\n",
        "    d_fake_loss = fake_loss(D_fake)\n",
        "\n",
        "    d_loss = d_real_loss + d_fake_loss\n",
        "    d_l += d_loss.item()\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "\n",
        "    #TRAIN THE GENERATOR (Train with fake images and flipped labels)\n",
        "    g_optimizer.zero_grad()\n",
        "\n",
        "    # Step 1: Zero gradients\n",
        "    # Step 2: Generate fake images from random noise (z)\n",
        "    # Step 3: Compute the discriminator losses on fake images using flipped labels!\n",
        "    # Step 4: Perform backprop and take optimizer step\n",
        "    z = torch.FloatTensor(batch_size, z_size).uniform_(-1, 1).to(device)\n",
        "\n",
        "    fake_images = G(z)\n",
        "\n",
        "    D_fake = D(fake_images)\n",
        "    g_loss = real_loss(D_fake)\n",
        "    g_l += g_loss.item()\n",
        "\n",
        "    g_loss.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "\n",
        "  # Print some loss stats\n",
        "  if epoch % print_every == 0:\n",
        "    print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(num_epochs)\n",
        "          + \"\\td_loss:\" + str(round(d_l/len(train_loader), 4))\n",
        "          + \"\\tg_loss:\" + str(round(g_l/len(train_loader), 4))\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsNSLUy-sbV",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Keep in mind:\n",
        "\n",
        "1. Always use a learning rate for discriminator higher than the generator.\n",
        "2. Keep training even if you see that the losses are going up.\n",
        "3. There are many variations with different loss functions which are worth exploring.\n",
        "4. If you get mode collapse, lower the learning rates.\n",
        "5. Adding noise to the training data helps make the model more stable.\n",
        "6. Label Smoothing: instead of making the labels as 1 make it 0.9\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Conditional GAN"
      ],
      "metadata": {
        "id": "S__H73zr5HEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.researchgate.net/profile/Gerasimos-Spanakis/publication/330474693/figure/fig1/AS:956606955139072@1605084279074/GAN-conditional-GAN-CGAN-and-auxiliary-classifier-GAN-ACGAN-architectures-where-x_Q320.jpg)"
      ],
      "metadata": {
        "id": "zybSTQqkKnRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Read Data"
      ],
      "metadata": {
        "id": "mOUeY__xaVYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize([32, 32]),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "# SVHN training datasets\n",
        "svhn_train = datasets.SVHN(root='data/', split='train', download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "num_workers = 0\n",
        "\n",
        "# build DataLoaders for SVHN dataset\n",
        "train_loader = torch.utils.data.DataLoader(dataset=svhn_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=num_workers)"
      ],
      "metadata": {
        "id": "a-Gea6EhaZcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Define helper functions"
      ],
      "metadata": {
        "id": "DMPxfzyWayQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(c_in, c_out, k_size=4, stride=2, pad=1, use_bn=True, transpose=False):\n",
        "  module = []\n",
        "  if transpose:\n",
        "    module.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
        "  else:\n",
        "    module.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
        "  if use_bn:\n",
        "    module.append(nn.BatchNorm2d(c_out))\n",
        "  return nn.Sequential(*module)"
      ],
      "metadata": {
        "id": "MVpI4qOla3vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Define Generator\n",
        "\n",
        "<font color='red'>**TODO:** Define Generator using achitecture of your choice</font>"
      ],
      "metadata": {
        "id": "qMM4WBWbXgS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU8Q-hacSger"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim=10, num_classes=10, label_embed_size=5, channels=3, conv_dim=64):\n",
        "    super(Generator, self).__init__()\n",
        "    self.image_size = 32\n",
        "    self.label_embedding = nn.Embedding(num_classes, label_embed_size)\n",
        "    self.l1 = conv_block(z_dim + label_embed_size, conv_dim * 4, pad=0, transpose=True)\n",
        "    self.l2 = conv_block(conv_dim * 4, conv_dim * 2, k_size=4, stride=2, pad=1, transpose=True)\n",
        "    self.l3 = conv_block(conv_dim * 2, conv_dim, k_size=4, stride=2, pad=1, transpose=True)\n",
        "    self.l4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(conv_dim, channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "      if isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x, condition):\n",
        "    x = x.reshape([x.shape[0], -1, 1, 1])\n",
        "    condition_embed = self.label_embedding(condition)\n",
        "    condition_embed = condition_embed.reshape([condition_embed.shape[0], -1, 1, 1])\n",
        "    x = torch.cat((x, condition_embed), dim=1)\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.l4(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Define Discriminator\n",
        "\n",
        "<font color='red'>**TODO:** Define Discriminator using achitecture of your choice</font>"
      ],
      "metadata": {
        "id": "0-F0-FzYXtu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, num_classes=10, channels=3, conv_dim=64):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.image_size = 32\n",
        "    self.condition_embedding = nn.Embedding(num_classes, self.image_size*self.image_size)\n",
        "    self.conv1 = conv_block(channels + 1, conv_dim, use_bn=False)\n",
        "    self.conv2 = conv_block(conv_dim, conv_dim * 2)\n",
        "    self.conv3 = conv_block(conv_dim * 2, conv_dim * 4)\n",
        "    self.conv4 = nn.Conv2d(conv_dim * 4, 1, kernel_size=4, stride=1, padding=0)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "\n",
        "      if isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x, condition):\n",
        "    alpha = 0.2\n",
        "    condition_embed = self.condition_embedding(condition)\n",
        "    condition_embed = condition_embed.reshape([condition_embed.shape[0], 1, self.image_size, self.image_size])\n",
        "    x = torch.cat((x, condition_embed), dim=1)\n",
        "    x = F.leaky_relu(self.conv1(x), alpha)\n",
        "    x = F.leaky_relu(self.conv2(x), alpha)\n",
        "    x = F.leaky_relu(self.conv3(x), alpha)\n",
        "    x = self.conv4(x)\n",
        "    return x.squeeze()"
      ],
      "metadata": {
        "id": "ubM2XPwAXyN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Assemble a cGAN"
      ],
      "metadata": {
        "id": "4Wi5y3v95qiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define discriminator and generator\n",
        "z_dim = 10\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "G = Generator(z_dim=z_dim, num_classes=10, label_embed_size=5, channels=3).to(device)\n",
        "D = Discriminator(num_classes=10, channels=3).to(device)\n",
        "\n",
        "#print the models summary\n",
        "print(D)\n",
        "print()\n",
        "print(G)"
      ],
      "metadata": {
        "id": "sEVm_ro7cOYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Define optimizer and criterion"
      ],
      "metadata": {
        "id": "ix0Gkl3M5avr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "g_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=2e-5)\n",
        "d_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=2e-5)\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "0wDpF1MyeWkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7 Training conditional GAN (training loop)\n",
        "\n",
        "\n",
        "<font color='red'>**TODO:** Train conditional GAN</font>\n"
      ],
      "metadata": {
        "id": "sMHzU0UsgdOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_epochs = 5\n",
        "\n",
        "# Labels\n",
        "real_label = torch.FloatTensor(batch_size).uniform_(0.9, 1).to(device) # torch.ones(batch_size)\n",
        "fake_label = torch.FloatTensor(batch_size).uniform_(0, 0.1).to(device)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  G.train()\n",
        "  D.train()\n",
        "  for batch_i, (x_real, y_real) in enumerate(train_loader):\n",
        "    batch_size = x_real.size(0)\n",
        "    x_real = x_real.to(device)\n",
        "    y_real = y_real.to(device)\n",
        "\n",
        "    # TODO\n",
        "    # TRAIN THE DISCRIMINATOR\n",
        "    # Step 1: Zero gradients (zero_grad)\n",
        "    d_optimizer.zero_grad()\n",
        "    # Step 2: Train with real images\n",
        "    real_output = D(x_real, y_real)\n",
        "    # Step 3: Compute the discriminator losses on real images\n",
        "    d_real_loss = criterion(real_output, real_label[:batch_size])\n",
        "\n",
        "    # Step 4: Train with fake images\n",
        "    z = torch.randn(batch_size, z_dim, device=device)\n",
        "    x_fake = G(z, y_real)\n",
        "    fake_output = D(x_fake.detach(), y_real)  # Detach to prevent generator update\n",
        "    d_fake_loss = criterion(fake_output, fake_label[:batch_size])\n",
        "\n",
        "    # Step 5: Generate fake images and move x to GPU, if available\n",
        "\n",
        "    # Step 6: Compute the discriminator losses on fake images\n",
        "    d_loss = d_real_loss + d_fake_loss\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "    # Step 7: add up loss and perform backprop\n",
        "\n",
        "\n",
        "    #TRAIN THE GENERATOR\n",
        "    # Step 1: Zero gradients\n",
        "    g_optimizer.zero_grad()\n",
        "    # Step 2: Generate fake images from random noise (z) and condition (y)\n",
        "    # Step 3: Compute the discriminator losses on fake images using flipped labels (labels -- true/fake)\n",
        "    # Step 4: Perform backprop and take optimizer step\n",
        "    z = torch.randn(batch_size, z_dim, device=device)\n",
        "    x_fake = G(z, y_real)\n",
        "    fake_output = D(x_fake, y_real)  # No detach, update G\n",
        "    g_loss = criterion(fake_output, real_label[:batch_size])  # Flipped labels\n",
        "\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "  # Print the loss for each epoch\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "dQZdP1b6ch41"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}