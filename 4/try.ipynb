{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CelebA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Устройство для обучения\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Параметры\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "latent_dim = 100\n",
    "num_attributes = 40  # Количество атрибутов в CelebA\n",
    "\n",
    "# Трансформации изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Загрузка CelebA датасета\n",
    "train_dataset = CelebA(root='./data', split='train', download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Проверка загрузки данных\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_attributes, img_size=64, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_attributes = num_attributes\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_attributes, 128 * 8 * 4 * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (128 * 8, 4, 4)),\n",
    "            nn.ConvTranspose2d(128 * 8, 128 * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128 * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128 * 4, 128 * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128 * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128 * 2, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, a):\n",
    "        # Concatenate latent vector and attribute vector\n",
    "        input = torch.cat((z, a), 1)\n",
    "        img = self.model(input)\n",
    "        return img\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size=64, channels=3, num_attributes=40):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        self.num_attributes = num_attributes\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels + num_attributes, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, a):\n",
    "        # Reshape attribute vector to feature map\n",
    "        a = a.view(a.size(0), a.size(1), 1, 1)\n",
    "        a = a.repeat(1, 1, img.size(2), img.size(3))\n",
    "        # Concatenate attribute map with image\n",
    "        img = torch.cat((img, a), 1)\n",
    "        validity = self.model(img)\n",
    "        return validity\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Инициализация моделей\n",
    "generator = Generator(latent_dim, num_attributes).to(device)\n",
    "discriminator = Discriminator(img_size=image_size, channels=3, num_attributes=num_attributes).to(device)\n",
    "\n",
    "# Оптимизаторы\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Функция потерь\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Функция для сохранения изображений\n",
    "def save_images(epoch, path='images'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    z = torch.randn(batch_size, latent_dim).to(device)\n",
    "    a = torch.randint(0, 2, (batch_size, num_attributes)).float().to(device)\n",
    "    gen_imgs = generator(z, a)\n",
    "    gen_imgs = gen_imgs.view(gen_imgs.size(0), 3, image_size, image_size)\n",
    "    gen_imgs = (gen_imgs * 0.5) + 0.5  # Unnormalize\n",
    "    torchvision.utils.save_image(gen_imgs, f\"{path}/{epoch}.png\", nrow=8, normalize=True)\n",
    "\n",
    "# Функция для обучения\n",
    "def train_cgan(generator, discriminator, train_loader, optimizer_G, optimizer_D, adversarial_loss, num_epochs=50):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (imgs, attrs) in enumerate(train_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            attrs = attrs.float().to(device)\n",
    "            batch_size = imgs.size(0)\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Sample noise and labels as generator input\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            gen_imgs = generator(z, attrs)\n",
    "\n",
    "            # Loss for real images\n",
    "            real_loss = adversarial_loss(discriminator(imgs, attrs), valid)\n",
    "            # Loss for fake images\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), attrs), fake)\n",
    "            # Total discriminator loss\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            g_loss = adversarial_loss(discriminator(gen_imgs, attrs), valid)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Print loss values\n",
    "            if i % 100 == 0:\n",
    "                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(train_loader)}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "        # Save losses\n",
    "        G_losses.append(g_loss.item())\n",
    "        D_losses.append(d_loss.item())\n",
    "\n",
    "        # Save images\n",
    "        save_images(epoch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}